{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (5.1.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import nlp\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import json\n",
    "from os import listdir\n",
    "import glob\n",
    "from scipy import spatial\n",
    "import spacy\n",
    "from flask import Flask, render_template, request\n",
    "import pandas as pd\n",
    "import docx2txt\n",
    "from output import *\n",
    "import nltk\n",
    "import re\n",
    "import subprocess  # noqa: S404\n",
    "app = Flask(__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass a third parameter(flag) as 1 in the match_profile() in order to get your recommendations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "c4827ed5e92d47eaa059096e4edea8afcd2a6e06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/atharvapatil/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/atharvapatil/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/atharvapatil/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/atharvapatil/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/atharvapatil/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:3333\n",
      " * Running on http://10.5.58.134:3333\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "10.5.58.134 - - [15/Apr/2023 22:32:06] \"GET / HTTP/1.1\" 200 -\n",
      "10.5.58.134 - - [15/Apr/2023 22:32:06] \"GET /static/stylesheets/style.css HTTP/1.1\" 200 -\n",
      "10.5.58.134 - - [15/Apr/2023 22:32:06] \"GET /static/stylesheets/job3.jpg HTTP/1.1\" 200 -\n",
      "10.5.58.134 - - [15/Apr/2023 22:32:06] \"GET /static/stylesheets/job2.jpg HTTP/1.1\" 200 -\n",
      "10.5.58.134 - - [15/Apr/2023 22:32:06] \"GET /static/stylesheets/job.jpg HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Html css', 'Html css java', 'css java', 'css java Network', 'java Network', 'java Network Engineer', 'Network Engineer']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tk/clwg0rpj6zx5g0mbxx7509j80000gn/T/ipykernel_39688/1588998610.py:553: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  userdomain=userdomain.append(dictionary,ignore_index=True)\n",
      "/var/folders/tk/clwg0rpj6zx5g0mbxx7509j80000gn/T/ipykernel_39688/1588998610.py:553: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  userdomain=userdomain.append(dictionary,ignore_index=True)\n",
      "/var/folders/tk/clwg0rpj6zx5g0mbxx7509j80000gn/T/ipykernel_39688/1588998610.py:562: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  userlanguages=userlanguages.append(dictionary,ignore_index=True)\n",
      "/var/folders/tk/clwg0rpj6zx5g0mbxx7509j80000gn/T/ipykernel_39688/1588998610.py:570: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  userframeworks=userframeworks.append(dictionary,ignore_index=True)\n",
      "/var/folders/tk/clwg0rpj6zx5g0mbxx7509j80000gn/T/ipykernel_39688/1588998610.py:570: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  userframeworks=userframeworks.append(dictionary,ignore_index=True)\n",
      "/var/folders/tk/clwg0rpj6zx5g0mbxx7509j80000gn/T/ipykernel_39688/1588998610.py:578: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  userplatforms=userplatforms.append(dictionary,ignore_index=True)\n",
      "/var/folders/tk/clwg0rpj6zx5g0mbxx7509j80000gn/T/ipykernel_39688/1588998610.py:578: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  userplatforms=userplatforms.append(dictionary,ignore_index=True)\n",
      "/var/folders/tk/clwg0rpj6zx5g0mbxx7509j80000gn/T/ipykernel_39688/1588998610.py:586: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  userdatabases=userdatabases.append(dictionary,ignore_index=True)\n",
      "/var/folders/tk/clwg0rpj6zx5g0mbxx7509j80000gn/T/ipykernel_39688/1588998610.py:586: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  userdatabases=userdatabases.append(dictionary,ignore_index=True)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/scipy/spatial/distance.py:622: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "/var/folders/tk/clwg0rpj6zx5g0mbxx7509j80000gn/T/ipykernel_39688/1588998610.py:639: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  rows=rows.append(row.iloc[0])\n",
      "/var/folders/tk/clwg0rpj6zx5g0mbxx7509j80000gn/T/ipykernel_39688/1588998610.py:639: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  rows=rows.append(row.iloc[0])\n",
      "/var/folders/tk/clwg0rpj6zx5g0mbxx7509j80000gn/T/ipykernel_39688/1588998610.py:639: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  rows=rows.append(row.iloc[0])\n",
      "/var/folders/tk/clwg0rpj6zx5g0mbxx7509j80000gn/T/ipykernel_39688/1588998610.py:639: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  rows=rows.append(row.iloc[0])\n",
      "/var/folders/tk/clwg0rpj6zx5g0mbxx7509j80000gn/T/ipykernel_39688/1588998610.py:639: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  rows=rows.append(row.iloc[0])\n",
      "/var/folders/tk/clwg0rpj6zx5g0mbxx7509j80000gn/T/ipykernel_39688/1588998610.py:639: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  rows=rows.append(row.iloc[0])\n",
      "/var/folders/tk/clwg0rpj6zx5g0mbxx7509j80000gn/T/ipykernel_39688/1588998610.py:639: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  rows=rows.append(row.iloc[0])\n",
      "/var/folders/tk/clwg0rpj6zx5g0mbxx7509j80000gn/T/ipykernel_39688/1588998610.py:639: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  rows=rows.append(row.iloc[0])\n",
      "/var/folders/tk/clwg0rpj6zx5g0mbxx7509j80000gn/T/ipykernel_39688/1588998610.py:639: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  rows=rows.append(row.iloc[0])\n",
      "/var/folders/tk/clwg0rpj6zx5g0mbxx7509j80000gn/T/ipykernel_39688/1588998610.py:639: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  rows=rows.append(row.iloc[0])\n",
      "10.5.58.134 - - [15/Apr/2023 22:32:32] \"POST /upload HTTP/1.1\" 200 -\n",
      "10.5.58.134 - - [15/Apr/2023 22:32:32] \"\u001b[36mGET /static/stylesheets/style.css HTTP/1.1\u001b[0m\" 304 -\n",
      "10.5.58.134 - - [15/Apr/2023 22:32:32] \"\u001b[36mGET /static/stylesheets/job.jpg HTTP/1.1\u001b[0m\" 304 -\n",
      "10.5.58.134 - - [15/Apr/2023 22:32:32] \"\u001b[36mGET /static/stylesheets/job2.jpg HTTP/1.1\u001b[0m\" 304 -\n",
      "10.5.58.134 - - [15/Apr/2023 22:32:32] \"\u001b[36mGET /static/stylesheets/job3.jpg HTTP/1.1\u001b[0m\" 304 -\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(arr1,arr2):\n",
    "    ans=1- spatial.distance.cosine(arr1,arr2)\n",
    "    if(np.isnan(ans)):\n",
    "        return 0\n",
    "    else:\n",
    "        return ans\n",
    "class job_postings:    \n",
    "    def __init__(self,link):\n",
    "        self.df2=pd.read_csv(link)\n",
    "        self.training_range=int(len(self.df2.loc[:,'uniq_id']))\n",
    "    def check_threshold(threshold,ele):\n",
    "        if(ele[0]!=threshold[0][0] and abs(ele[1]-threshold[0][1])<0.03):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    def categorize_jobs(self):\n",
    "        # #Predefined categories\n",
    "        #Compare similarities of word embeddings\n",
    "        nlp=spacy.load('en_core_web_lg')\n",
    "        job_id=self.df2.loc[:,'uniq_id'].tolist()[:self.training_range]\n",
    "        job_titles=self.df2.loc[:,'jobtitle'].tolist()[:self.training_range]\n",
    "        job_descriptions=self.df2.loc[:,'jobdescription'].tolist()[:self.training_range]\n",
    "        final_cat=pd.DataFrame(index=job_id)\n",
    "        #categories=['Network Engineer','Application Development','Big Data','Data Analyst','Software Developer','DevOps','Software Testing','Front End','Back End','Full Stack','Web Development','Information Security','Mobile developer','System Administrator','Business Analyst','Manager','Cloud']\n",
    "        categories=['Network Engineer','Full stack','QA/Test Developer','Enterprise application','DevOps','Mobile Developer','Back End','Database Administrator(DBA)','Front End','Game developer','System Administrator','Data Scientist','Business analyst','Sales professional','Product Manager','Information Security','Software Developer/Java Developer','Web Developer','Cloud Computing']\n",
    "        for category in categories:\n",
    "            final_cat[category]=np.nan\n",
    "        for job_t_d in list(zip(job_id,job_titles,job_descriptions)):\n",
    "            id_job=job_t_d[0]\n",
    "            job_i=job_t_d[1]\n",
    "            job_d=job_t_d[2]\n",
    "            job_title=nlp(job_i.lower())\n",
    "            job_description=nlp(job_d.lower())\n",
    "            match_cat_title=dict()\n",
    "            match_cat_description=dict()\n",
    "            for category in categories:\n",
    "                word=nlp(category.lower())\n",
    "                match_cat_title[category]=job_title.similarity(word)\n",
    "                match_cat_description[category]=job_description.similarity(word)\n",
    "            match_cat_title=sorted(match_cat_title.items(),key=lambda x:x[1],reverse=True)\n",
    "            match_cat_description=sorted(match_cat_description.items(),key=lambda x:x[1],reverse=True)\n",
    "\n",
    "\n",
    "            #a represents max\n",
    "            #if(match_cat_title[0][1]>0.5 or match_cat_description[0][1]>0.5):\n",
    "            a=match_cat_title[0]\n",
    "            #print(a)\n",
    "            match_cat_description=list(filter(lambda x: self.check_threshold(match_cat_title,x),match_cat_description))\n",
    "            if(len(match_cat_description)!=0):\n",
    "                print(match_cat_description)\n",
    "                print(id_job)\n",
    "                #b=match_cat_description[0]\n",
    "                final_cat.loc[id_job,a[0]]=1\n",
    "                match_cat_description.extend([(match_cat_title[0][0],1)])\n",
    "                sum_proportion=sum([x[1] for x in match_cat_description])\n",
    "                for ele in match_cat_description:\n",
    "                    final_cat.loc[id_job,ele[0]]=ele[1]/sum_proportion\n",
    "            else:\n",
    "                print(id_job)\n",
    "                final_cat.loc[id_job,a[0]]=1\n",
    "        return final_cat\n",
    "    def clean_skills(self):\n",
    "        extracted_skills=dict()\n",
    "        job_skills=np.asarray(self.df2.loc[:,\"skills\"])\n",
    "        for i in range(self.training_range):\n",
    "            #print(i)\n",
    "            #Method 1: Manual pre-processing\n",
    "            job_id=self.df2.iloc[i,-1]\n",
    "            #Method 2:Using NLTK\n",
    "            tokenizer=nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "            #print(job_skills[i])\n",
    "            if(pd.isnull(job_skills[i])):\n",
    "                continue\n",
    "            stopwords_list=stopwords.words(\"english\")\n",
    "            tokens=re.split(\"|\".join([\",\",\" and\",\"/\",\" AND\",\" or\",\" OR\",\";\"]),job_skills[i])\n",
    "            tokens=list(set(tokens))\n",
    "            extracted_skills[job_id]=[]\n",
    "            extracted_skills[job_id].extend(tokens)\n",
    "        return extracted_skills\n",
    "    def extract_skills(self,extracted_skills):\n",
    "        df_languages=pd.read_excel('./data/job_profile/languages.xlsx')\n",
    "        df_frameworks=pd.read_csv(\"./data/job_profile/frameworks.csv\")\n",
    "        df_database=pd.read_csv(\"./data/job_profile/database.csv\")\n",
    "        df_os=pd.read_csv(\"./data/job_profile/operating_systems.csv\")\n",
    "        df_plat=pd.read_csv(\"./data/job_profile/platforms.csv\")\n",
    "        frameworks=df_frameworks.iloc[:,1].tolist()\n",
    "        frameworks=[x.lower().strip() for x in frameworks]\n",
    "        #frameworks=[str(x).split(\",\")[0] for x in df_frameworks.iloc[:,1]]\n",
    "        languages=list(df_languages.iloc[:,0])\n",
    "        languages=[x.lower().strip() for x in languages]\n",
    "        #frameworks=[x.lower().strip().split('\\t')[0] for x in frameworks]\n",
    "        databases=df_database.iloc[:,0].tolist()\n",
    "        databases=[x.lower().strip() for x in databases]\n",
    "        op_systems=df_os.iloc[:,0].tolist()\n",
    "        op_systems=[x.lower().strip() for x in op_systems]\n",
    "        platforms=df_plat.iloc[:,1].tolist()\n",
    "        #print(platforms)\n",
    "        platforms=[x.lower().strip() for x in platforms]\n",
    "        #print(frameworks)\n",
    "        new_extracted=dict()\n",
    "        for ele in extracted_skills.keys():\n",
    "            final_lang=''\n",
    "            final_frame=''\n",
    "            final_others=''\n",
    "            final_database=''\n",
    "            final_plat=''\n",
    "            final_os=''\n",
    "            #print(extracted_skills[ele])\n",
    "            for skill in extracted_skills[ele]:\n",
    "                skill_base=skill.lower().strip()\n",
    "                #print(skill_base)\n",
    "                if(skill_base in languages):\n",
    "                    if(final_lang==''):\n",
    "                        final_lang=skill_base\n",
    "                    else:\n",
    "                        final_lang=final_lang+\",\"+skill_base\n",
    "                elif(skill_base in frameworks):\n",
    "                    if(final_frame==''):\n",
    "                        final_frame=skill_base\n",
    "                    else:\n",
    "                        final_frame=final_frame+\",\"+skill_base\n",
    "                elif(skill_base in databases):\n",
    "                    if(final_database==''):\n",
    "                        final_database=skill_base\n",
    "                    else:\n",
    "                        final_database=final_database+\",\"+skill_base\n",
    "                elif(skill_base in op_systems):\n",
    "                    if(final_os==''):\n",
    "                        final_os=skill_base\n",
    "                    else:\n",
    "                        final_os=final_os+\",\"+skill_base\n",
    "                elif(skill_base in platforms):\n",
    "                    if(final_plat==''):\n",
    "                        final_plat=skill_base\n",
    "                    else:\n",
    "                        final_plat=final_plat+\",\"+skill_base\n",
    "                else:\n",
    "                    if(final_others==''):\n",
    "                        final_others=skill_base\n",
    "                    else:\n",
    "                        final_others=final_others+\",\"+skill_base\n",
    "            new_extracted[ele]=[final_lang,final_frame,final_database,final_os,final_plat,final_others]\n",
    "        print((list(new_extracted.items()))[:100])\n",
    "        for ele,describe in list(zip(self.df2.loc[:,'uniq_id'],self.df2.loc[:,'jobdescription'].tolist()))[:self.training_range]:\n",
    "            doc=nlp(describe)\n",
    "            final_lang=''\n",
    "            final_frame=''\n",
    "            final_others=''\n",
    "            final_database=''\n",
    "            final_plat=''\n",
    "            final_os=''\n",
    "            for ent in doc.ents:\n",
    "                word=ent.text\n",
    "                word=word.lower().strip()\n",
    "                if(word in languages and word not in final_lang and word not in new_extracted[ele][0].split(\",\")):\n",
    "                    if(final_lang==''):\n",
    "                        final_lang=word\n",
    "                    else:\n",
    "                        final_lang=final_lang+\",\"+word\n",
    "                elif(word in frameworks and word not in final_frame and word not in new_extracted[ele][1].split(\",\")):\n",
    "                    if(final_frame==''):\n",
    "                        final_frame=word\n",
    "                    else:\n",
    "                        final_frame=final_frame+\",\"+word\n",
    "                elif(word in databases and word not in final_database and word not in new_extracted[ele][2].split(\",\")):\n",
    "                    if(final_database==''):\n",
    "                        final_database=word\n",
    "                    else:\n",
    "                        final_database=final_database+\",\"+word\n",
    "                elif(word in op_systems and word not in final_os and word not in new_extracted[ele][3].split(\",\")):\n",
    "                    if(final_os==''):\n",
    "                        final_os=word\n",
    "                    else:\n",
    "                        final_os=final_os+\",\"+word\n",
    "                elif(word in platforms and word not in final_plat and word not in new_extracted[ele][4].split(\",\")):\n",
    "                    if(final_plat==''):\n",
    "                        final_plat=word\n",
    "                    else:\n",
    "                        final_plat=final_plat+\",\"+word\n",
    "                else:\n",
    "                    if(final_others==''):\n",
    "                        final_others=word\n",
    "                    else:\n",
    "                        final_others=final_others+\",\"+word\n",
    "            if(final_lang!=''):\n",
    "                new_extracted[ele][0]+=\",\"+final_lang\n",
    "            if(final_frame!=''):\n",
    "                new_extracted[ele][1]+=\",\"+final_frame\n",
    "            if(final_database!=''):\n",
    "                new_extracted[ele][2]+=\",\"+final_database\n",
    "            if(final_os!=''):\n",
    "                new_extracted[ele][3]+=\",\"+final_os\n",
    "            if(final_plat!=''):\n",
    "                new_extracted[ele][4]+=\",\"+final_plat\n",
    "            if(final_others!=''):\n",
    "                new_extracted[ele][5]+=\",\"+final_others\n",
    "            #new_extracted[ele]=[final_lang,final_frame,final_database,final_os,final_plat,final_others]\n",
    "        extracted_skills_df=pd.DataFrame.from_dict(new_extracted,orient='index',columns=['Language','Framework','Database','OS','Platform','Others'])\n",
    "        return extracted_skills_df\n",
    "    def create_job_profile(self,extracted_skills_df,domain_df):\n",
    "        job_id=extracted_skills_df.index.tolist()\n",
    "        languages_df=pd.DataFrame(index=job_id)\n",
    "        platforms_df=pd.DataFrame(index=job_id)\n",
    "        frameworks_df=pd.DataFrame(index=job_id)\n",
    "        databases_df=pd.DataFrame(index=job_id)\n",
    "        \n",
    "        for job,lang,frame,plat,datab in list(zip(job_id,extracted_skills_df.loc[:,'Language'].tolist(),extracted_skills_df.loc[:,'Framework'].tolist(),extracted_skills_df.loc[:,'Platform'].tolist(),extracted_skills_df.loc[:,'Database'].tolist())):\n",
    "            #Languages\n",
    "            l=lang.split(\",\")\n",
    "            if(lang!=np.nan or lang!=''):\n",
    "                for ele in l:\n",
    "                    if(ele==''):\n",
    "                        continue\n",
    "                    if(ele not in languages_df.columns):\n",
    "                        #languages.append(ele)\n",
    "                        languages_df[ele]=np.nan\n",
    "                    languages_df.loc[job,ele]=1\n",
    "            \n",
    "            #Frameworks\n",
    "            l=frame.split(\",\")\n",
    "            if(frame!=np.nan or frame!=''):\n",
    "                for ele in l:\n",
    "                    if(ele==''):\n",
    "                        continue\n",
    "                    if(ele not in frameworks_df.columns):\n",
    "                        #languages.append(ele)\n",
    "                        frameworks_df[ele]=np.nan\n",
    "                    frameworks_df.loc[job,ele]=1\n",
    "\n",
    "            #Platforms\n",
    "            l=plat.split(\",\")\n",
    "            if(plat!=np.nan or plat!=''):\n",
    "                for ele in l:\n",
    "                    if(ele==''):\n",
    "                        continue\n",
    "                    if(ele not in platforms_df.columns):\n",
    "                        #languages.append(ele)\n",
    "                        platforms_df[ele]=np.nan\n",
    "                    platforms_df.loc[job,ele]=1\n",
    "            \n",
    "            #Databases\n",
    "            l=datab.split(\",\")\n",
    "            if(datab!=np.nan or datab!=''):\n",
    "                for ele in l:\n",
    "                    if(ele==''):\n",
    "                        continue\n",
    "                    if(ele not in databases_df.columns):\n",
    "                        #languages.append(ele)\n",
    "                        databases_df[ele]=np.nan\n",
    "                    databases_df.loc[job,ele]=1\n",
    "        languages_df=languages_df.reindex_axis(sorted(languages_df.columns), axis=1)\n",
    "        frameworks_df=frameworks_df.reindex_axis(sorted(frameworks_df.columns), axis=1)\n",
    "        platforms_df=platforms_df.reindex_axis(sorted(platforms_df.columns), axis=1)\n",
    "        databases_df=databases_df.reindex_axis(sorted(databases_df.columns), axis=1)\n",
    "        domain_df=domain_df.reindex_axis(sorted(domain_df.columns), axis=1)\n",
    "        \n",
    "        languages_df.index.name=frameworks_df.index.name=platforms_df.index.name=databases_df.index.name=domain_df.index.name='uniq_id'\n",
    "        languages_df.to_csv(\"./data/job_profile/languages_job_profile.csv\")\n",
    "        frameworks_df.to_csv(\"./data/job_profile/frameworks_job_profile.csv\")\n",
    "        platforms_df.to_csv(\"./data/job_profile/platforms_job_profile.csv\")\n",
    "        databases_df.to_csv(\"./data/job_profile/databases_job_profile.csv\")\n",
    "        domain_df.to_csv(\"./data/job_profile/domain_job_profile.csv\")\n",
    "        print(languages_df.columns)\n",
    "        \n",
    "    def clean_common_profile(self,df_user,df_job,flag):\n",
    "        #Shift .net from languages to frameworks\n",
    "        if(flag=='Language'):\n",
    "            print(df_job.columns.tolist())\n",
    "            #bash and bash/shell\n",
    "            count=0\n",
    "            for ele in df_user.loc[:,'bash/shell']:\n",
    "                if(ele==1.0):\n",
    "                    df_user.ix[count,'bash']=1.0\n",
    "                count=count+1\n",
    "            df_user=df_user.drop('bash/shell',axis=1)\n",
    "            count=0\n",
    "            for ele in df_job.loc[:,'bash/shell']:\n",
    "                if(ele==1.0):\n",
    "                    df_job.ix[count,'bash']=1.0\n",
    "                count=count+1\n",
    "            df_job=df_job.drop('bash/shell',axis=1)\n",
    "\n",
    "        if(flag=='Framework'):\n",
    "            print(df_user.columns.tolist())\n",
    "            count=0\n",
    "            for ele in df_user.loc[:,'nodejs']:\n",
    "                if(ele==1.0):\n",
    "                    df_user.ix[count,'node.js']=1.0\n",
    "                count=count+1\n",
    "            df_user=df_user.drop('nodejs',axis=1)\n",
    "            count=0\n",
    "            for ele in df_job.loc[:,'nodejs']:\n",
    "                if(ele==1.0):\n",
    "                    df_job.ix[count,'node.js']=1.0\n",
    "                count=count+1\n",
    "            df_job=df_job.drop('nodejs',axis=1)\n",
    "            \n",
    "            count=0\n",
    "            for ele in df_user.loc[:,'angularjs']:\n",
    "                if(ele==1.0):\n",
    "                    df_user.ix[count,'angular']=1.0\n",
    "                count=count+1\n",
    "            df_user=df_user.drop('angularjs',axis=1)\n",
    "            count=0\n",
    "            for ele in df_job.loc[:,'angularjs']:\n",
    "                if(ele==1.0):\n",
    "                    df_job.ix[count,'angular']=1.0\n",
    "                count=count+1\n",
    "            df_job=df_job.drop('angularjs',axis=1)\n",
    "            \n",
    "        if(flag=='Platform'):\n",
    "            print(df_user.columns.tolist())\n",
    "        if(flag=='Database'):\n",
    "            print(df_user.columns.tolist())\n",
    "            count=0\n",
    "            for ele in df_user.loc[:,'microsoft sql server']:\n",
    "                if(ele==1.0):\n",
    "                    df_user.ix[count,'sql server']=1.0\n",
    "                count=count+1\n",
    "            df_user=df_user.drop('microsoft sql server',axis=1)\n",
    "            count=0\n",
    "            for ele in df_job.loc[:,'microsoft sql server']:\n",
    "                if(ele==1.0):\n",
    "                    df_job.ix[count,'sql server']=1.0\n",
    "                count=count+1\n",
    "            df_job=df_job.drop('microsoft sql server',axis=1)\n",
    "        return df_user,df_job\n",
    "\n",
    "    #Input is two dataframes    \n",
    "    def create_common_profile(self,job_profile_path,user_profile_path,output_path,flag=0):\n",
    "        if(flag==0):\n",
    "            #Domain\n",
    "            userprofile=pd.read_csv(user_profile_path+\"DevType.csv\",index_col='Respondent')\n",
    "            jobprofile=pd.read_csv(job_profile_path+\"domain_job_profile.csv\",index_col='Unnamed: 0')\n",
    "            print(\"Read from file\")\n",
    "            print(jobprofile.index)\n",
    "            #jobprofile=jobprofile.reset_index()\n",
    "            #userprofile=userprofile.reset_index()\n",
    "            userprofile.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "            jobprofile.drop('uniq_id', axis=1, inplace=True)\n",
    "            jobprofile.index.name='uniq_id'\n",
    "            print(\"index 2in domain\")\n",
    "            print(jobprofile.index)\n",
    "            #print(jobprofile.loc[:,'uniq_id'])\n",
    "            userprofile.rename(columns={'Product manager':'Product Manager','Back-end developer':'Back End','C-suite executive (CEO, CTO, etc.)':'C-suite executive','Data scientist or machine learning specialist':'Data Scientist','Database administrator':'Database Administrator(DBA)','Mobile developer':'Mobile Developer','Desktop or enterprise applications developer':'Enterprise application','DevOps specialist':'DevOps','Front-end developer':'Front End','Full-stack developer':'Full stack','Marketing or sales professional':'Sales professional','QA or test developer':'QA/Test Developer','System administrator':'System Administrator','Game or graphics developer':'Game developer'},inplace=True)\n",
    "            jobprofile.rename(columns={'Business analyst':'Data or business analyst'},inplace=True)\n",
    "            print(userprofile.columns)\n",
    "            print(jobprofile.columns)\n",
    "            print(\"index in domain\")\n",
    "            print(jobprofile.index)\n",
    "            #Present in userprofile but not in jobprofile\n",
    "            a=list(set(userprofile.columns)-set(jobprofile.columns))\n",
    "            print(a)\n",
    "            for i in a:\n",
    "                if(i!='Respondent'):\n",
    "                    jobprofile[i]=0\n",
    "            b=list(set(jobprofile.columns)-set(userprofile.columns))\n",
    "            print(b)\n",
    "            for i in b:\n",
    "                if(i!='uniq_id'):\n",
    "                    userprofile[i]=0\n",
    "            #userprofile=userprofile.set_index('Respondent')\n",
    "            #jobprofile=jobprofile.set_index('uniq_id')\n",
    "            userprofile=userprofile[sorted(userprofile.columns.tolist())]\n",
    "            jobprofile=jobprofile[sorted(jobprofile.columns.tolist())]\n",
    "            #Exclude \n",
    "\n",
    "            print(userprofile.columns==jobprofile.columns)\n",
    "\n",
    "            print(userprofile.columns)\n",
    "            print(jobprofile.columns)\n",
    "            userprofile=userprofile[userprofile.columns.tolist()]\n",
    "            jobprofile=jobprofile[jobprofile.columns.tolist()]\n",
    "            userprofile.to_csv(output_path+\"domain_user_profile.csv\")\n",
    "            jobprofile.to_csv(output_path+\"domain_job_profile.csv\")\n",
    "\n",
    "            #Languages\n",
    "            df_user=pd.read_csv(user_profile_path+\"LanguageWorkedWith.csv\",index_col='Respondent')\n",
    "            df_job=pd.read_csv(job_profile_path+\"languages_job_profile.csv\",index_col=0)\n",
    "            df_job.index.name='uniq_id'\n",
    "            print(\"index is\")\n",
    "            print(df_job.index)\n",
    "            print(df_user.columns)\n",
    "            print(df_job.columns)\n",
    "            df_user.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "            #df_job.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "            df_job.rename(columns={'visual basic .net':'vb.net'},inplace=True)\n",
    "            df_user.columns=list(map(lambda x:x.lower(),df_user.columns))\n",
    "            df_job.columns=list(map(lambda x:x.lower(),df_job.columns))\n",
    "            columns_to_add=[]\n",
    "            a=list(set(df_user.columns)-(set(df_job.columns)))\n",
    "            print(a)\n",
    "            for i in a:\n",
    "                if(i!='Respondent'):\n",
    "                    df_job[i]=0        \n",
    "            b=list(set(df_job.columns)-set(df_user.columns))\n",
    "            print(b)\n",
    "            for i in b:\n",
    "                if(i!='uniq_id'):\n",
    "                    df_user[i]=0\n",
    "            print(df_job.index)        \n",
    "            df_user=df_user[sorted(df_user.columns.tolist())]\n",
    "            df_job=df_job[sorted(df_job.columns.tolist())]\n",
    "            #df_user=userprofile.reindex_axis(sorted(df_user.columns), axis=1)\n",
    "            #df_job=jobprofile.reindex_axis(sorted(df_job.columns), axis=1)\n",
    "            print(\"index 2\")\n",
    "            print(df_job.index)\n",
    "            print(len(set(df_user.columns).intersection(df_job.columns)),len(df_user.columns))\n",
    "            df_user,df_job=self.clean_common_profile(df_user,df_job,'Language')\n",
    "            print(\"language is\")\n",
    "            print(df_job.index[0])\n",
    "            print(df_job.loc[df_job.index[0],:])\n",
    "            df_user.to_csv(output_path+\"languages_profile_user.csv\")\n",
    "            df_job.to_csv(output_path+\"languages_profile_job.csv\")\n",
    "\n",
    "            #Frameworks\n",
    "            df_user=pd.read_csv(user_profile_path+\"FrameworkWorkedWith.csv\",index_col='Respondent')\n",
    "            df_job=pd.read_csv(job_profile_path+\"frameworks_job_profile.csv\",index_col=0) \n",
    "            df_job.index.name='uniq_id'\n",
    "            print(df_user.columns)\n",
    "            print(df_job.columns)\n",
    "            df_user.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "            #df_job.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "            #df_job.rename(columns={'visual basic .net':'vb.net'},inplace=True)\n",
    "            df_user.columns=list(map(lambda x:x.lower(),df_user.columns))\n",
    "            df_job.columns=list(map(lambda x:x.lower(),df_job.columns))\n",
    "\n",
    "            a=list(set(df_user.columns)-(set(df_job.columns)))\n",
    "            print(a)\n",
    "            for i in a:\n",
    "                if(i!='Respondent'):\n",
    "                    df_job[i]=0        \n",
    "            b=list(set(df_job.columns)-set(df_user.columns))\n",
    "            print(b)\n",
    "            for i in b:\n",
    "                if(i!='uniq_id'):\n",
    "                    df_user[i]=0\n",
    "            #userprofile=userprofile.reindex_axis(sorted(userprofile.columns), axis=1)\n",
    "            #jobprofile=jobprofile.reindex_axis(sorted(jobprofile.columns), axis=1)\n",
    "            df_user=df_user[sorted(df_user.columns.tolist())]\n",
    "            df_job=df_job[sorted(df_job.columns.tolist())]\n",
    "\n",
    "            print(len(set(df_user.columns).intersection(df_job.columns)),len(df_user.columns))\n",
    "            df_user,df_job=self.clean_common_profile(df_user,df_job,'Framework')   \n",
    "            df_user.to_csv(output_path+\"frameworks_profile_user.csv\")\n",
    "            df_job.to_csv(output_path+\"frameworks_profile_job.csv\")\n",
    "\n",
    "            #Platforms\n",
    "            df_user=pd.read_csv(user_profile_path+\"PlatformWorkedWith.csv\",index_col='Respondent')\n",
    "            df_job=pd.read_csv(job_profile_path+\"platforms_job_profile.csv\",index_col=0) \n",
    "            print(df_user.columns)\n",
    "            df_job.index.name='uniq_id'\n",
    "            print(df_job.columns)\n",
    "            df_user.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "            #df_job.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "            #df_job.rename(columns={'visual basic .net':'vb.net'},inplace=True)\n",
    "            df_user.columns=list(map(lambda x:x.lower(),df_user.columns))\n",
    "            df_job.columns=list(map(lambda x:x.lower(),df_job.columns))\n",
    "\n",
    "            a=list(set(df_user.columns)-(set(df_job.columns)))\n",
    "            print(a)\n",
    "            for i in a:\n",
    "                if(i!='Respondent'):\n",
    "                    df_job[i]=0\n",
    "            b=list(set(df_job.columns)-set(df_user.columns))\n",
    "            print(b)\n",
    "            for i in b:\n",
    "                if(i!='uniq_id'):\n",
    "                    df_user[i]=0\n",
    "            df_user=df_user[sorted(df_user.columns.tolist())]\n",
    "            df_job=df_job[sorted(df_job.columns.tolist())]\n",
    "\n",
    "            print(len(set(df_user.columns).intersection(df_job.columns)),len(df_user.columns))\n",
    "            df_user,df_job=self.clean_common_profile(df_user,df_job,'Platform')        \n",
    "            df_user.to_csv(output_path+\"platforms_profile_user.csv\")\n",
    "            df_job.to_csv(output_path+\"platforms_profile_job.csv\")\n",
    "\n",
    "            #Databases\n",
    "            df_user=pd.read_csv(user_profile_path+\"DatabaseWorkedWith.csv\",index_col='Respondent')\n",
    "            df_job=pd.read_csv(job_profile_path+\"databases_job_profile.csv\",index_col=0) \n",
    "            df_job.index.name='uniq_id'\n",
    "            print(df_user.columns)\n",
    "            print(df_job.columns)\n",
    "            df_user.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "            #df_job.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "            #df_job.rename(columns={'visual basic .net':'vb.net'},inplace=True)\n",
    "            df_user.columns=list(map(lambda x:x.lower(),df_user.columns))\n",
    "            df_job.columns=list(map(lambda x:x.lower(),df_job.columns))\n",
    "\n",
    "            a=list(set(df_user.columns)-(set(df_job.columns)))\n",
    "            print(a)\n",
    "            for i in a:\n",
    "                if(i!='Respondent'):\n",
    "                    df_job[i]=0\n",
    "            b=list(set(df_job.columns)-set(df_user.columns))\n",
    "            print(b)\n",
    "            for i in b:\n",
    "                if(i!='uniq_id'):\n",
    "                    df_user[i]=0\n",
    "            df_user=df_user[sorted(df_user.columns.tolist())]\n",
    "            df_job=df_job[sorted(df_job.columns.tolist())]\n",
    "\n",
    "            print(len(set(df_user.columns).intersection(df_job.columns)),len(df_user.columns))\n",
    "            df_user,df_job=self.clean_common_profile(df_user,df_job,'Database')        \n",
    "            df_user.to_csv(output_path+\"databases_profile_user.csv\")\n",
    "            df_job.to_csv(output_path+\"databases_profile_job.csv\")\n",
    "        #flag indicates that a new user profile\n",
    "    def match_profile(self,input_path,user_id,skillx,domainx,flag=0):\n",
    "        #Match a given user_id with all jobs in the database\n",
    "        \n",
    "        #Check if user id exists\n",
    "        df=pd.read_csv(input_path+\"domain_user_profile.csv\",index_col='Respondent')\n",
    "        #print(df.columns)\n",
    "        matches=dict()\n",
    "        if(flag==0):\n",
    "            if(user_id in df.index):\n",
    "                userdomain=df.loc[user_id,:]\n",
    "                #print(userdomain)\n",
    "                #If it does, retrieve the user profile from input_path\n",
    "                df=pd.read_csv(input_path+\"languages_profile_user.csv\",index_col='Respondent')\n",
    "                userlanguages=df.loc[user_id,:]\n",
    "\n",
    "                df=pd.read_csv(input_path+\"frameworks_profile_user.csv\",index_col='Respondent')\n",
    "                userframeworks=df.loc[user_id,:]\n",
    "\n",
    "                df=pd.read_csv(input_path+\"platforms_profile_user.csv\",index_col='Respondent')\n",
    "                userplatforms=df.loc[user_id,:]\n",
    "\n",
    "                df=pd.read_csv(input_path+\"databases_profile_user.csv\",index_col='Respondent')\n",
    "                userdatabases=df.loc[user_id,:]\n",
    "\n",
    "                userdomain=np.asarray(userdomain.fillna(0))\n",
    "                userlanguages=np.asarray(userlanguages.fillna(0))\n",
    "                userframeworks=np.asarray(userframeworks.fillna(0))\n",
    "                userplatforms=np.asarray(userplatforms.fillna(0))\n",
    "                userdatabases=np.asarray(userdatabases.fillna(0))\n",
    "                #print(userdomain)\n",
    "            else:\n",
    "                print(\"error! user id not in Dataset\")\n",
    "            #If it doesn't,take user profile as input\n",
    "        else:\n",
    "            skills=skillx\n",
    "            domains=''\n",
    "            flag=1\n",
    "            \n",
    "            #domains=list(map(lambda x:x.lower(),domains))\n",
    "            skills=list(map(lambda x:x.lower(),skills))                \n",
    "\n",
    "            userdomain=pd.DataFrame(columns=df.columns)\n",
    "            dictionary=dict()\n",
    "            for domain in domains:\n",
    "                dictionary[domain]=1.0\n",
    "            userdomain=userdomain.append(dictionary,ignore_index=True)\n",
    "\n",
    "\n",
    "            df=pd.read_csv(input_path+\"languages_profile_user.csv\",index_col='Respondent')\n",
    "            userlanguages=pd.DataFrame(columns=df.columns)\n",
    "            dictionary=dict()\n",
    "            for skill in skills:\n",
    "                if(skill in df.columns):\n",
    "                    dictionary[skill]=1.0\n",
    "            userlanguages=userlanguages.append(dictionary,ignore_index=True)\n",
    "\n",
    "            df=pd.read_csv(input_path+\"frameworks_profile_user.csv\",index_col='Respondent')\n",
    "            userframeworks=pd.DataFrame(columns=df.columns)\n",
    "            dictionary=dict()\n",
    "            for skill in skills:\n",
    "                if(skill in df.columns):\n",
    "                    dictionary[skill]=1.0\n",
    "            userframeworks=userframeworks.append(dictionary,ignore_index=True)\n",
    "\n",
    "            df=pd.read_csv(input_path+\"platforms_profile_user.csv\",index_col='Respondent')\n",
    "            userplatforms=pd.DataFrame(columns=df.columns)                \n",
    "            dictionary=dict()\n",
    "            for skill in skills:\n",
    "                if(skill in df.columns):\n",
    "                    dictionary[skill]=1.0\n",
    "            userplatforms=userplatforms.append(dictionary,ignore_index=True)\n",
    "\n",
    "            df=pd.read_csv(input_path+\"databases_profile_user.csv\",index_col='Respondent')\n",
    "            userdatabases=pd.DataFrame(columns=df.columns)               \n",
    "            dictionary=dict()\n",
    "            for skill in skills:\n",
    "                if(skill in df.columns):\n",
    "                    dictionary[skill]=1.0\n",
    "            userdatabases=userdatabases.append(dictionary,ignore_index=True)\n",
    "            #print(userdomain)\n",
    "            userdomain=np.asarray(userdomain.iloc[0,:].fillna(0))\n",
    "            userlanguages=np.asarray(userlanguages.iloc[0,:].fillna(0))\n",
    "            userframeworks=np.asarray(userframeworks.iloc[0,:].fillna(0))\n",
    "            userplatforms=np.asarray(userplatforms.iloc[0,:].fillna(0))\n",
    "            userdatabases=np.asarray(userdatabases.iloc[0,:].fillna(0))\n",
    "                \n",
    "        jobdomain=pd.read_csv(input_path+\"domain_job_profile.csv\",index_col='uniq_id')\n",
    "        joblanguages=pd.read_csv(input_path+'languages_profile_job.csv',index_col='uniq_id')\n",
    "        jobframeworks=pd.read_csv(input_path+'frameworks_profile_job.csv',index_col='uniq_id')\n",
    "        jobplatforms=pd.read_csv(input_path+'platforms_profile_job.csv',index_col='uniq_id')\n",
    "        jobdatabases=pd.read_csv(input_path+'databases_profile_job.csv',index_col='uniq_id')\n",
    "        #print(len(jobdomain.index),len(joblanguages.index))\n",
    "        for i in jobdomain.index:\n",
    "            #print(i)\n",
    "            domain=jobdomain.loc[i,:].fillna(0)\n",
    "            language=joblanguages.loc[i,:].fillna(0)\n",
    "            framework=jobframeworks.loc[i,:].fillna(0)\n",
    "            platform=jobplatforms.loc[i,:].fillna(0)\n",
    "            database=jobdatabases.loc[i,:].fillna(0)\n",
    "            job_id=str(i)\n",
    "            domain=np.asarray(domain)\n",
    "            language=np.asarray(language)\n",
    "            framework=np.asarray(framework)\n",
    "            platform=np.asarray(platform)\n",
    "            database=np.asarray(database)\n",
    "            #print(len(domain),len(userdomain))\n",
    "            score=(0.7*cosine_similarity(domain,userdomain))+(0.3*(cosine_similarity(language,userlanguages)+cosine_similarity(framework,userframeworks)+cosine_similarity(platform,userplatforms)+cosine_similarity(database,userdatabases)))\n",
    "            matches[job_id]=score\n",
    "            score=(0.7*cosine_similarity(domain,userdomain))+(0.3*(cosine_similarity(language,userlanguages)+cosine_similarity(framework,userframeworks)+cosine_similarity(platform,userplatforms)+cosine_similarity(database,userdatabases)))\n",
    "            #Initializing job profiles for later access\n",
    "            self.job_domain=domain\n",
    "            self.job_language=language\n",
    "            self.job_framework=framework\n",
    "            self.job_platform=platform\n",
    "            self.job_database=database\n",
    "            \n",
    "            self.user_domain=userdomain\n",
    "            self.user_language=userlanguages\n",
    "            self.user_framework=userframeworks\n",
    "            self.user_platform=userplatforms\n",
    "            self.user_database=userdatabases\n",
    "        matches=sorted(matches.items(),key=lambda x:x[1],reverse=True)\n",
    "        \n",
    "        recommendations=matches[:10]\n",
    "        #print(\"recommendations are\")\n",
    "        #print(recommendations)\n",
    "        rows=pd.DataFrame(columns=self.df2.columns)\n",
    "        count=0\n",
    "        for i in recommendations:\n",
    "            row=self.df2[self.df2['uniq_id']==i[0]]\n",
    "            #rows[count]=np.asarray(row.values.T.tolist()[0])\n",
    "            rows=rows.append(row.iloc[0])\n",
    "            count=count+1\n",
    "            #print(row)\n",
    "        return rows\n",
    "            \n",
    "\n",
    "obj=job_postings(\"./data/dice_com-job_us_sample.csv\")\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "SKILLS_DB = [\n",
    "    'docker',\n",
    "    'kubernetes',\n",
    "    'python',\n",
    "    'word',\n",
    "    'excel',\n",
    "    'English',\n",
    "    'nlp',\n",
    "    'html',\n",
    "    'css',\n",
    "    'java',\n",
    "    'c++',\n",
    "    'git',\n",
    "    'flask',\n",
    "    'nodejs'\n",
    "]\n",
    "\n",
    "DOMAIN_DB = [\n",
    "    'Web Developer',\n",
    "    'Software Developer/Java Developer',\n",
    "    'Network Engineer',\n",
    "    'Cloud Computing'\n",
    "]\n",
    "\n",
    "skills = []\n",
    " \n",
    "def extract_text_from_docx(docx_path):\n",
    "    txt = docx2txt.process(docx_path)\n",
    "    if txt:\n",
    "        return txt.replace('\\t', ' ')\n",
    "    return None\n",
    "\n",
    "def extract_skills(input_text):\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    word_tokens = nltk.tokenize.word_tokenize(input_text)\n",
    " \n",
    "    # remove the stop words\n",
    "    filtered_tokens = [w for w in word_tokens if w not in stop_words]\n",
    " \n",
    "    # remove the punctuation\n",
    "    filtered_tokens = [w for w in word_tokens if w.isalpha()]\n",
    " \n",
    "    # generate bigrams and trigrams (such as artificial intelligence)\n",
    "    bigrams_trigrams = list(map(' '.join, nltk.everygrams(filtered_tokens, 2, 3)))\n",
    " \n",
    "    # we create a set to keep the results in.\n",
    "    found_skills = set()\n",
    " \n",
    "    # we search for each token in our skills database\n",
    "    for token in filtered_tokens:\n",
    "        if token.lower() in SKILLS_DB:\n",
    "            found_skills.add(token)\n",
    "    \n",
    "    print(bigrams_trigrams)  #comment this before production\n",
    "\n",
    "    for x in DOMAIN_DB:\n",
    "        if x in bigrams_trigrams:\n",
    "            domain_des=x\n",
    " \n",
    "    # we search for each bigram and trigram in our skills database\n",
    "    for ngram in bigrams_trigrams:\n",
    "        if ngram.lower() in SKILLS_DB:\n",
    "            found_skills.add(ngram)\n",
    " \n",
    "    return found_skills,domain_des\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/upload', methods=['POST'])\n",
    "def upload():\n",
    "    file = request.files['file']\n",
    "    text = extract_text_from_docx(file)\n",
    "    skills,domain = extract_skills(text)\n",
    "    \n",
    "    default_skills = \",\".join(skills)\n",
    "    #print(df.to_html())\n",
    "    #return df.to_html()\n",
    "    #df.to_html('templates\\\\table.html')\n",
    "    #return default\n",
    "    skillx = default_skills\n",
    "    domainx = domain\n",
    "    #Pass a third parameter(flag) as 1 in order to get your recommendations!\n",
    "    rows=obj.match_profile(\"./data/\",3,skillx,domainx,1)\n",
    "    #rows.drop(['uniq_id'], axis=1)\n",
    "    del rows['uniq_id']\n",
    "    del rows['jobdescription']\n",
    "    del rows['jobid']\n",
    "    del rows['shift']\n",
    "    del rows['site_name']\n",
    "    rows.rename(columns={'advertiserurl': 'Link_To_Apply', 'employmenttype_jobstatus': 'Job_Type','joblocation_address':'Location', 'jobtitle':'Job_Title','company':'Company','skills':'Skills','postdate':'Update'}, inplace=True)\n",
    "    # print(\"type of rows\",type(rows))\n",
    "    rows.describe()\n",
    "    rows.reset_index(drop=True, inplace=True)\n",
    "    # rows.index = np.arange(1, len(rows) + 1)\n",
    "    rows.index = rows.index + 1\n",
    "    table_html = rows.to_html(border=1, render_links=True, escape=False, col_space=3)\n",
    "\n",
    "    #return str1 + default + str2  \n",
    "    return render_template('output.html',output_final = table_html)  \n",
    "\n",
    "\n",
    "app.run(host=\"0.0.0.0\",port=3333)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
